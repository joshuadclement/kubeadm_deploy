#!/bin/bash

mkdir -p \
			customfiles/home/user/.kube \
			customfiles/root/.kube \
			customfiles/etc/kubernetes/helm_values \
			customfiles/etc/kubernetes/custom_manifests \
			customfiles/etc/kubernetes/manually_apply \
			customfiles/etc/kubernetes/custom_manifests/docker_registry \
			customfiles/etc/kubernetes/custom_manifests/acmedns

tftpget() {
	if [[ -f "${1}" ]]; then
		echo "${1} Exists locally, continuing..."
	else
		echo "fetching ${1} from the tftp server..."
		# Fetch the file
		tftp 10.0.0.1 -c get kubernetes/"${1}"
		# Separate the relative path from the filename
		FILEPATH=$(echo "${1}" | sed 's/\/[^/]*$//')
		# If there is a '/' anywhere in the relative path of the file,
		if [[ "${FILEPATH}" != "${1}" ]]; then
			FILENAME=$(echo "${1}" | awk -F / '{print $NF}')
			mkdir -p "${FILEPATH}"
			mv "${FILENAME}" "${1}"
		fi
		echo "done"
	fi
}


# Helm
##########################################################################################
echo "Installing helm"
HELM_VERSION=$(jq < env.json -r '.versions.helm')
tftpget downloads/helm-v"${HELM_VERSION}"-linux-amd64.tar.gz
mkdir helm
tar -xzf downloads/helm-v"${HELM_VERSION}"-linux-amd64.tar.gz -C helm
chmod 0751 helm/linux-amd64/helm
cp helm/linux-amd64/helm customfiles/usr/local/bin
rm -rf helm

# Kubeadm init
##########################################################################################
tftpget templates/kubeadm-config.yaml.j2
j2 -f json templates/kubeadm-config.yaml.j2 env.json > customfiles/etc/kubernetes/kubeadm-config.yaml
kubeadm init --config customfiles/etc/kubernetes/kubeadm-config.yaml 2>&1

# Copy kubectl config
cp /etc/kubernetes/admin.conf customfiles/home/user/.kube/config
chown user:user customfiles/home/user/.kube/config
cp /etc/kubernetes/admin.conf customfiles/root/.kube/config
rsync -rp customfiles/ /
chown user:user /home/user/.kube/config

# Install calico
##########################################################################################
echo "Installing calico"
CALICO_VERSION=$(jq < env.json -r '.versions.calico')
helm repo add projectcalico https://docs.tigera.io/calico/charts
tftpget templates/calico_values.yaml.j2
j2 -f json templates/calico_values.yaml.j2 env.json > customfiles/etc/kubernetes/helm_values/calico_values.yaml
kubectl create namespace tigera-operator
helm install calico projectcalico/tigera-operator --version v"${CALICO_VERSION}" -f customfiles/etc/kubernetes/helm_values/calico_values.yaml --namespace tigera-operator --wait

tftpget downloads/calicoctl-linux-amd64
chmod +x downloads/calicoctl-linux-amd64
cp downloads/calicoctl-linux-amd64 customfiles/usr/local/bin/calicoctl

rsync -rp customfiles/ /

# Wait for calico to be ready
while true; do
	kubectl --namespace calico-system wait --for=condition=Available deployments/calico-kube-controllers --timeout=10m
	if [[ $? == 0 ]]; then
		break
	else
		echo "calico-kube-controllers doesn't exist yet, sleeping before trying again"
		sleep 10
	fi
done

# Configure IPpools
echo "Configuring IP pools"
tftpget templates/calico_ip_pools.yaml.j2
j2 -f json templates/calico_ip_pools.yaml.j2 env.json > customfiles/etc/kubernetes/custom_manifests/calico_ip_pools.yaml
calicoctl delete ippool default-ipv4-ippool
calicoctl apply -f customfiles/etc/kubernetes/custom_manifests/calico_ip_pools.yaml
# Delete pods with an IP address in the cluster range
# These should just be calico and coredns pods, and their deployments/daemonsets will schedule new ones.
# The new ones will be inside the new, correct IPpools.
CLUSTER_RANGE=$(jq < env.json -r '.cluster_range')
kubectl get pods --all-namespaces -o wide | grepcidr "${CLUSTER_RANGE}" | while read p; do
	NAMESPACE=$(echo "${p}" | awk '{print $1}')
	PODNAME=$(echo "${p}" | awk '{print $2}')
	kubectl delete pod -n "${NAMESPACE}" "${PODNAME}"
done
# Restart the calico components so that they're aware of the new ippools
# This ensures that calico will add routes to pods scheduled with addresses from the new ippools.
kubectl get daemonset -n calico-system | tail -n +2 | while read d; do
	DSNAME=$(echo "${d}" | awk '{print $1}')
	kubectl rollout restart daemonset -n calico-system "${DSNAME}"
done

# Set taint on control plane correctly
HOSTNAME=$(hostname -s)
kubectl taint nodes "${HOSTNAME}" node-role.kubernetes.io/control-plane:NoSchedule-
kubectl taint nodes "${HOSTNAME}" --overwrite node-role.kubernetes.io/control-plane:PreferNoSchedule


# Make token and join command for worker nodes
##########################################################################################
kubeadm token create --ttl 0 --print-join-command > customfiles/etc/kubernetes/join-command
chmod 0600 customfiles/etc/kubernetes/join-command

# Copy all files from customfiles into system folders
rsync -rp customfiles/ /
echo "Ready for worker nodes to join cluster"
# At this point, worker nodes will be able to join the cluster by copying the join command via ssh

create_namespaces_for_manifest() {
	yq e '.metadata.namespace' < "${1}" | grep -v -- --- | grep -v null | while read n; do
		kubectl create namespace "${n}"
	done
}

# Apply network policies
##########################################################################################
echo "Applying network policies"
tftpget templates/network_policies.yaml.j2
j2 -f json templates/network_policies.yaml.j2 env.json > customfiles/etc/kubernetes/custom_manifests/network_policies.yaml
# Create all the namespaces that need to be there in order to apply this manifest
create_namespaces_for_manifest customfiles/etc/kubernetes/custom_manifests/network_policies.yaml
kubectl apply -f customfiles/etc/kubernetes/custom_manifests/network_policies.yaml


# Ingress
##########################################################################################
# Before it can be applied, there has to be a worker node with a public IP address
while true; do
	FOUND=$(kubectl get nodes -o json | jq -r '.items[].metadata.labels.hasPublicIP' | while read p; do
		if [[ $p == true ]]; then
			echo "true"
			break
		fi
	done)
	if [[ $FOUND == true ]]; then
		echo "There is now a node with a public IP address in the cluster. Proceeding to install ingress controller."
		break
	fi
	sleep 10
	echo "Waiting for a node with a public IP to be available..."
done

# Ingress controller
echo "Installing ingress controller"
helm repo add k8s-ingress-nginx https://kubernetes.github.io/ingress-nginx
tftpget templates/ingress_values.yaml
cp templates/ingress_values.yaml customfiles/etc/kubernetes/helm_values/ingress_values.yaml
helm install -f customfiles/etc/kubernetes/helm_values/ingress_values.yaml ingress-nginx k8s-ingress-nginx/ingress-nginx --namespace ingress-nginx --create-namespace

# Cert manager
echo "Installing cert manager"
CERT_MANAGER_VERSION=$(jq < env.json -r '.versions.cert_manager')
helm repo add jetstack https://charts.jetstack.io
tftpget templates/cert_manager_values.yaml
cp templates/cert_manager_values.yaml customfiles/etc/kubernetes/helm_values/cert_manager_values.yaml
helm install -f customfiles/etc/kubernetes/helm_values/cert_manager_values.yaml cert-manager jetstack/cert-manager --namespace cert-manager --create-namespace --version v"${CERT_MANAGER_VERSION}"

tftpget templates/cluster_issuer.yaml.j2
j2 -f json templates/cluster_issuer.yaml.j2 env.json > customfiles/etc/kubernetes/custom_manifests/cluster_issuer.yaml
kubectl apply -f customfiles/etc/kubernetes/custom_manifests/cluster_issuer.yaml

# Apply backed up secrets if exist
TESTING=$(jq < env.json -r '.testing')
if [[ $TESTING == "true" ]]; then
	SECRET_SUBFOLDER="test"
else
	SECRET_SUBFOLDER="prod"
fi
tftpget secrets/"${SECRET_SUBFOLDER}"/backup_certs.yaml
if [[ -s secrets/"${SECRET_SUBFOLDER}"/backup_certs.yaml ]]; then
	echo "Applying saved certificates from a previous cluster"
	create_namespaces_for_manifest secrets/"${SECRET_SUBFOLDER}"/backup_certs.yaml
	kubectl apply -f secrets/"${SECRET_SUBFOLDER}"/backup_certs.yaml
fi

# Wait
kubectl --namespace ingress-nginx wait --for=condition=Available deployments/ingress-nginx-controller --timeout=10m

# Docker registry
##########################################################################################

echo "Installing docker registry"
tftpget secrets/"${SECRET_SUBFOLDER}"/docker_registry_admin_password
DOCKER_REGISTRY_ADMIN_PASSWORD=$(cat secrets/"${SECRET_SUBFOLDER}"/docker_registry_admin_password)
htpasswd -b -c customfiles/etc/kubernetes/custom_manifests/docker_registry/docker_htpasswd admin "${DOCKER_REGISTRY_ADMIN_PASSWORD}"
export DOCKER_REGISTRY_HTPASSWD_B64=$(cat customfiles/etc/kubernetes/custom_manifests/docker_registry/docker_htpasswd | base64 -w 0)

export DOCKER_REGISTRY_LOGIN_STRING_B64=$(echo -n "admin:${DOCKER_REGISTRY_ADMIN_PASSWORD}" | base64 -w 0)
tftpget templates/docker_registry_login_config.json.j2
j2 -f json templates/docker_registry_login_config.json.j2 env.json > customfiles/etc/kubernetes/custom_manifests/docker_registry/docker_registry_login_config.json
export DOCKER_REGISTRY_LOGIN_CONFIG_B64=$(cat customfiles/etc/kubernetes/custom_manifests/docker_registry/docker_registry_login_config.json | base64 -w 0)

tftpget templates/docker_registry.yaml.j2
j2 -f json templates/docker_registry.yaml.j2 env.json > customfiles/etc/kubernetes/custom_manifests/docker_registry.yaml
kubectl apply -f customfiles/etc/kubernetes/custom_manifests/docker_registry.yaml

# Acmedns
##########################################################################################

echo "Installing acmedns"
tftpget templates/deploy_acmedns.yaml.j2
j2 -f json templates/deploy_acmedns.yaml.j2 env.json > customfiles/etc/kubernetes/custom_manifests/deploy_acmedns.yaml
# Deploy the acmedns server and wait for it to be available
kubectl apply -f customfiles/etc/kubernetes/custom_manifests/deploy_acmedns.yaml
kubectl --namespace acme-dns wait --for=condition=Available deployments/acme-dns --timeout=10m
# sleep just to make sure acmedns is actually ready to get requests
sleep 10

# Apply backed up secrets if exist
for filename in "acmedns_sdpods.json" "acmedns_sdtestpods.json"; do
	tftpget secrets/"${SECRET_SUBFOLDER}"/"${filename}"
	if [[ -s secrets/"${SECRET_SUBFOLDER}"/"${filename}" ]]; then
		echo "Found backup of ${filename} from a previous cluster, copying it."
		cp secrets/"${SECRET_SUBFOLDER}"/"${filename}" customfiles/etc/kubernetes/custom_manifests/acmedns/"${filename}"
	else
		echo "No backup of ${filename} found, registering the new subdomain with the acmedns server."
		# If there is no backed up registration, then curl the server to create one
		ACMEDNS_HOSTNAME=$(jq < env.json -r '.acmedns.hostname')
		TRUSTED_NETWORK_CIDR=$(jq < env.json -r '.trusted_network_cidr')
		VLAN_NETWORK_CIDR=$(jq < env.json -r '.vlan_network_cidr')
		CLUSTER_RANGE=$(jq < env.json -r '.cluster_range')
		ACMEDNS_PUBLIC_IP=$(jq < env.json -r '{n: .acmedns.node_hostname, p: .public_network_configs | to_entries[]} | select(.p.key == .n) | .p.value.address')
		curl -X POST http://"${ACMEDNS_HOSTNAME}"/register \
				 -H "Content-Type: application/json" \
				 --data '{"allowfrom": ["'"${ACMEDNS_PUBLIC_IP}"'/32", "127.0.0.0/8", "'"${TRUSTED_NETWORK_CIDR}"'", "'"${VLAN_NETWORK_CIDR}"'"]}' | \
			tee "${filename}"
		# If this is run for the first time, it will just copy the registration file in,
		# but it's possible to run this again after teardown.
		# It's important to avoid losing the registration files because then new DNS records will be needed.
		if [[ -s /etc/kubernetes/custom_manifests/acmedns/"${filename}" ]]; then
			read -p "${filename} already found, are you sure you want to replace it with the newly generated one:
"$(cat "${filename}")"
[y/N]" -n 1 -r
			if [[ $REPLY == y ]]; then
				echo "Replacing saved ${filename} with the newly generated registration file."
				mv "${filename}" customfiles/etc/kubernetes/custom_manifests/acmedns/"${filename}"
			else
				echo "Not replacing the existing ${filename}"
			fi
		else
			mv "${filename}" customfiles/etc/kubernetes/custom_manifests/acmedns/"${filename}"
		fi
	fi
done

echo "Creating acmedns issuers for sciencedata and sciencedata-dev from the registration files"

PODS_DOMAIN=$(jq < env.json -r '.user_pods.ingress_domain')
TEST_PODS_DOMAIN=$(jq < env.json -r '.user_pods.test_ingress_domain')
export SD_REGISTRATION=$(jq < customfiles/etc/kubernetes/custom_manifests/acmedns/acmedns_sdpods.json -c '{"'"${PODS_DOMAIN}"'": .}' | base64 -w 0)
export SD_TEST_REGISTRATION=$(jq < customfiles/etc/kubernetes/custom_manifests/acmedns/acmedns_sdtestpods.json -c '{"'"${TEST_PODS_DOMAIN}"'": .}' | base64 -w 0)
tftpget templates/acmedns_issuer.yaml.j2
j2 -f json templates/acmedns_issuer.yaml.j2 env.json > customfiles/etc/kubernetes/custom_manifests/acmedns_issuer.yaml
kubectl apply -f customfiles/etc/kubernetes/custom_manifests/acmedns_issuer.yaml

# User pods backend
################################################################################

echo "Installing user pods backend"

tftpget templates/user_pods_backend.yaml.j2
j2 -f json templates/user_pods_backend.yaml.j2 env.json > customfiles/etc/kubernetes/custom_manifests/user_pods_backend.yaml
kubectl apply -f customfiles/etc/kubernetes/custom_manifests/user_pods_backend.yaml

rsync -rp customfiles/ /
